'''
1st to find the distribution of coma & period on the entire dataset. for example: the dataset has 1000 utterances, with 100 comma and 200 period.

So to make it balances when dividing:

train set: 800 utterances, 80 commas, 160 periods

dev/test set: 100 utterances, 10 commas, 20 periods

because we want to keep the conversation order, so you have to choose the train/dev/set base using 1st the documents ID, 

2nd ensure the chosen documents have roughly the same ratio of # utterance/#words, #comma, # period

conversation-level split is better
'''

import json
import numpy as np
import pandas as pd
from fast_ml.model_development import train_valid_test_split
import os
import argparse
import csv
import shutil

parser = argparse.ArgumentParser(description='arguments')

parser.add_argument('--data-path',type=str,default='',help='path to the directory that stores cleaned_all.txt generated by the shell script')


def part1(args):
    # with open('./preprocess/dataset/cleaned_new_all_2speaker_reorder_with_timestamp.txt', 'r') as f:  #TODO change path to file
    #     texts = f.readlines()
    with open(os.path.join(args.data_path,'cleaned_all.txt'),'r') as f:
        texts = f.readlines()

    dic = {}

    #divide each conversation can use a {}  key-value: conversation name -  [utterance IDs]  store in json
    for line in texts:
        key = line.split(' ')[0][:-14]
        if key not in dic.keys():
            dic[key] = [[], [], []] #index , utterance, <c/> <s/>
        dic[key][0].append(texts.index(line))
        utterance = ' '.join(line.split(' ')[3:])
        dic[key][1].append(utterance)
        dic[key][2].append([utterance.count('<c/>'), utterance.count('<s/>')])

    # with open("./preprocess/dataset/mml2speaker_reorder_sample.json", "w") as outfile: 
    #     json.dump(dic, outfile, ensure_ascii=False)
    with open(os.path.join(args.data_path,"sgh_sample.json"), "w") as outfile: 
        json.dump(dic, outfile, ensure_ascii=False)

    #   calculate the sum of each conversation's #utterance #comma #period, and the total sum
    dic = {}
    with open(os.path.join(args.data_path,"sgh_sample.json"),'r') as infile: 
        dic = json.load(infile)
    dic_summary = {}
    for key in dic.keys():
        comma, period = np.sum(np.array(dic[key][2]), axis = 0)
        dic_summary[key] = np.array([len(dic[key][0]), comma, period]).tolist()

    dic_summary['total'] = [len(list(dic_summary.keys()))] + np.sum(np.array(list(dic_summary.values())), axis = 0).tolist()
    with open(os.path.join(args.data_path,"sgh_sample_summary.json"), 'w') as out: 
        json.dump(dic_summary,out, indent=4, separators=[', ', ': '])


    #  split into 8:1:1 of conversations store in txt format


    # conversation name + utterance num + start index + end index + comma + period
    with open(os.path.join(args.data_path,"sgh_sample.json"),'r') as infile1: 
        dic = json.load(infile1)
    with open (os.path.join(args.data_path,"sgh_sample_summary.json"), 'r') as infile2: 
        dic_summary = json.load(infile2)
    if list(dic.keys()) != list(dic_summary.keys())[:-1]: #last key in dic_summary is the summary info 
        raise Exception("The keys in two dictionaries aren't the same.")
    with open(os.path.join(args.data_path,'sgh_dataframe.txt'), 'w') as f: 
        f.write("conversation,utterance_num,start_index,end_index,comma_num,period_num\n")
        for key in list(dic.keys()):
            line = ','.join([key, str(dic_summary[key][0]), str(dic[key][0][0]), str(dic[key][0][-1]), str(dic_summary[key][1]), str(dic_summary[key][2])])
            f.write(line+'\n')

    read_file = pd.read_csv(os.path.join(args.data_path,'sgh_dataframe.txt'))
    read_file.to_csv(os.path.join(args.data_path,'sgh_dataframe.csv'))

def part2(args):
    with open (os.path.join(args.data_path,"sgh_sample_summary.json"), 'r') as infile2:
        dic_summary = json.load(infile2)
    total_utter, total_comma, total_period = dic_summary['total'][1], dic_summary['total'][2], dic_summary['total'][3]

    

    input_file = os.path.join(args.data_path,'sgh_dataframe.csv')
    output_file = os.path.join(args.data_path,'sgh_dataframe_1.csv')

    # Open input and output files
    with open(input_file, 'r') as f_in, open(output_file, 'w', newline='') as f_out:
        reader = csv.reader(f_in)
        writer = csv.writer(f_out)

        # Write header row with "index" column
        header = next(reader)
        writer.writerow(['index'] + header[1:])

        # Write data rows with index numbers
        for i, row in enumerate(reader):
            writer.writerow([i] + row[1:])

    df = pd.read_csv(os.path.join(args.data_path,'sgh_dataframe_1.csv')) #add 'index' before , in the first line 
    df['rand'] = df['index']
    for i in range(50):
        X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df, target = 'rand', train_size=0.8, valid_size=0.1, test_size=0.1) #target doesn't matter
        train_utter, train_comma, train_period = X_train[['utterance_num', 'comma_num', 'period_num']].sum(axis=0)
        val_utter, val_comma, val_period = X_valid[['utterance_num', 'comma_num', 'period_num']].sum(axis=0)
        test_utter, test_comma, test_period = X_test[['utterance_num', 'comma_num', 'period_num']].sum(axis=0)
        tr_u, v_u, te_u = round(float(train_utter)/float(total_utter),1), round(float(val_utter)/float(total_utter),1), round(float(test_utter)/float(total_utter),1)
        tr_c, v_c, te_c = round(float(train_comma)/float(total_comma),1), round(float(val_comma)/float(total_comma),1), round(float(test_comma)/float(total_comma),1)
        tr_p, v_p, te_p = round(float(train_period)/float(total_period),1), round(float(val_period)/float(total_period),1), round(float(test_period)/float(total_period),1)
        if tr_u == 0.8 and tr_c == 0.8 and tr_p == 0.8 and v_u == 0.1 and v_c ==0.1 and v_p ==0.1 and te_u == 0.1 and te_c == 0.1 and te_p == 0.1:
            print('found train val test sets that satisfy in round {}!'.format(i))
            X_train.to_csv(os.path.join(args.data_path,"sgh_raw_train.csv"))
            X_test.to_csv(os.path.join(args.data_path,"sgh_raw_test.csv"))
            X_valid.to_csv(os.path.join(args.data_path,"sgh_raw_val.csv"))
            break
        if i==49: print('not found satisfying sets in 50 rounds.')


    input_file = os.path.join(args.data_path,'cleaned_all.txt')
    output_file = os.path.join(args.data_path,'cleaned_all.csv')


    shutil.copy(input_file, output_file)

    # Insert 'utterance' before the first line of the output file
    with open(output_file, 'r+') as f:
        content = f.read()
        f.seek(0, 0)
        f.write("utterance\n" + content)

    with open(output_file, 'r') as f:
        lines = f.readlines()

    with open(output_file, 'w', newline='') as f:
        writer = csv.writer(f)
        for line in lines:
            writer.writerow([line.strip()])



    # generate the final train test val txt files
    df_all = pd.read_csv(os.path.join(args.data_path,'cleaned_all.csv'))  #for this file can just replicate the txt file, add utterance in the first line and change it to .csv

    train = pd.read_csv(os.path.join(args.data_path,"sgh_raw_train.csv"))
    with open(os.path.join(args.data_path,"sgh_train.txt"),'w') as f1:
        for iter in range(train.shape[0]):
            for utter_idx in range(train.loc[iter]['start_index'], train.loc[iter]['end_index']+1):
                f1.write(df_all.loc[utter_idx]['utterance']+'\n')
            

    test = pd.read_csv(os.path.join(args.data_path,"sgh_raw_test.csv"))
    with open(os.path.join(args.data_path,"sgh_test.txt"),'w') as f2:
        for iter in range(test.shape[0]):
            for utter_idx in range(test.loc[iter]['start_index'], test.loc[iter]['end_index']+1):
                f2.write(df_all.loc[utter_idx]['utterance']+'\n')


    val = pd.read_csv(os.path.join(args.data_path,"sgh_raw_val.csv"))
    with open(os.path.join(args.data_path,"sgh_val.txt"),'w') as f3:
        for iter in range(val.shape[0]):
            for utter_idx in range(val.loc[iter]['start_index'], val.loc[iter]['end_index']+1):
                f3.write(df_all.loc[utter_idx]['utterance']+'\n')



if __name__ == '__main__':

    args = parser.parse_args()
    path = args.data_path
    part1(args)
    part2(args)
